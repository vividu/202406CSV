{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vividu/202406CSV/blob/main/GB886_IV_6_LogisticRegressionExampleConfusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "In this tutorial, we introduce one of the most basic *binary classifiers*: **Logistic Regression**.\n",
        "\n",
        "Assume we are given predictors $X_i$ and outcome variables $Y_i \\in \\{0,1\\}$.  Remember that we are trying to assess\n",
        "$$\n",
        "p(x_0)= \\mathbb{P}(Y=1|X=x_0).\n",
        "$$\n",
        "A sensible prediction for given data $x_0$ is then:\n",
        "$$\n",
        "\\hat{Y} = \\left\\{\\begin{array}{l} 0 \\text{ if }p(x_0) \\leq 0.5,\\\\ 1\\text{ if }p(x_0) > 0.5. \\end{array}\\right.\n",
        "$$\n",
        "\n",
        "For instance, let's assume that $X$ is two-dimensional consisting of height and weight, and that we attempt to predict sex $Y$---say 1 for male and 0 for females.  Hence, $p(x)$ is the probability for male given $x =\\{\\text{height, weight}\\}$, and the probability of being female is $1-p(x).$\n",
        "\n",
        "This is the idea behind **Logistic regression** imposes a functional form on $p(x)$:\n",
        "$$\n",
        "p(x) = \\frac{e^{x\\beta}}{1+e^{x\\beta}} = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}}{1+e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}}\n",
        "$$\n",
        "\n",
        "For instance, in our male/female example, we would have:\n",
        "$$\n",
        "\\text{Proability Male} = p(\\underbrace{\\text{height},\\text{weight}}_x)=\\frac{e^{\\beta_0+\\beta_1\\times\\text{height}+\\beta_2\\times\\text{weight}}}{1+e^{\\beta_0+\\beta_1\\times\\text{height}+\\beta_2\\times\\text{weight}}}.\n",
        "$$\n",
        "\n",
        "Our predictive modeling process then works as follows:\n",
        "* We need to collect/find training data $(y_1,x_1),\\,(y_2,x_2),\\ldots,\\,(y_N,x_N)$, in our example consisting of information on individual's sex and their height and weight.\n",
        "* Based on the data, we use *Maximum Likelihood Estimation* to fit the model, i.e., to find an *estimate* fore the parameter vector $\\beta=(\\beta_0,\\beta_1,\\beta_2)'$.\n",
        "* Then, we can predict the probability of being male/female for a given (height,weight) combination.\n"
      ],
      "metadata": {
        "id": "Fc-m__Y8s4Y3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "## Application\n",
        "\n",
        "As always, let's start with importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "Let's look at the dataset from a [study by Caroline Davis](https://www.sciencedirect.com/science/article/pii/019566639090096Q) that has data on the height and weight of 200 individuals.\n",
        "\n",
        "Let's load the data:"
      ],
      "metadata": {
        "id": "O6HjArdXvU7c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r49dAbuLNki0"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT2dwPKCNvzF"
      },
      "source": [
        "hw_data = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_IV_3_Davis.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's take a look:"
      ],
      "metadata": {
        "id": "k6QRgH0olWJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hw_data.head()"
      ],
      "metadata": {
        "id": "fhXb30UClZ8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's recast the `sex` variable as a dummy variable, because that's the input for logistic regression packages."
      ],
      "metadata": {
        "id": "SbcJviOblafe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l073F2eMOCBC"
      },
      "source": [
        "hw_data['sex'] = pd.get_dummies(hw_data['sex'],drop_first=True)\n",
        "hw_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's visualize the data by plotting a scatterplot of height (in cm, on x-axis) and weight (in kg, on y-axis) and showing the labels by coloring the points:"
      ],
      "metadata": {
        "id": "AKDdfNl2lwB-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F3aCUWYJqYf"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "# plot x,y data with c as the color vector, set the line width of the markers to 0\n",
        "ax.scatter(hw_data['height'], hw_data['weight'], c= hw_data['sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, sex male is associated with larger height and weight.\n",
        "\n",
        "### Fitting Logistic Regression Model\n",
        "\n",
        "Let's run our classification model, where once again we use statsmodels for convenient output:"
      ],
      "metadata": {
        "id": "KIGZVmK9mDAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = hw_data[['height','weight']]\n",
        "X = sm.add_constant(X)\n",
        "y = hw_data[['sex']]\n",
        "\n",
        "logit_mod = sm.Logit(y, X)\n",
        "logit_mod_res = logit_mod.fit()\n",
        "\n",
        "print(logit_mod_res.summary())"
      ],
      "metadata": {
        "id": "llq7DMGIPMsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we see that the coefficients for height and weight are positive. That means taller people are more likely to be labeled as male. But how do we think about the actual interpretation?\n",
        "\n",
        "To illustrate, consider two individuals that are 55kg and one is 170cm tall (person A), the other one is 175 cm tall (person B). Let's get the predicted probablity for them being sex male from our model:"
      ],
      "metadata": {
        "id": "DU2c0V6Bmyu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logit_mod_res.predict([1,170,55]) #person A"
      ],
      "metadata": {
        "id": "rhYgvp5TSwT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit_mod_res.predict([1,175,55]) #person B"
      ],
      "metadata": {
        "id": "VNpfEPsVS70G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we indicated, we can transpate predicted probabilities into odds. Let's evaluate the \"oddsn for being male\" for person A and person B:"
      ],
      "metadata": {
        "id": "LWFDMvgAndst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oddsA = logit_mod_res.predict([1,170,55])/(1-logit_mod_res.predict([1,170,55]))\n",
        "oddsA"
      ],
      "metadata": {
        "id": "BX23tEk_nuit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oddsB = logit_mod_res.predict([1,175,55])/(1-logit_mod_res.predict([1,175,55]))\n",
        "oddsB"
      ],
      "metadata": {
        "id": "kn4JG_-PS_6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we said that in logistic regression, the log-odds  are linear. So let's calculate the log-odds for person B minus the log-odds for person A divided by 5, because person B is 5 cm taller:"
      ],
      "metadata": {
        "id": "OzXLh2bvn72Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(np.log(oddsB)-np.log(oddsA))/5"
      ],
      "metadata": {
        "id": "NgVj8A5ATX2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and that's exactly our coefficient $\\beta_1$. So, the interpretation is as follows: For every cm of height, the log-odds for $Y=1$ increase by $\\beta_1$. Or, equivakently, for every cm of height, the odds increase by a factor of $e^{\\beta_1}$:"
      ],
      "metadata": {
        "id": "pnsLcUCnoPAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp((np.log(oddsB)-np.log(oddsA))/5)"
      ],
      "metadata": {
        "id": "vEFJjXuspERs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "\n",
        "Let's predict the probability for being male in our sample:"
      ],
      "metadata": {
        "id": "Bm_vI6-IonJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pres = logit_mod_res.get_prediction()\n",
        "pres.summary_frame()"
      ],
      "metadata": {
        "id": "KupOAtv-SpVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare out predicted probabiloties (x-axis) and the actual outcomes (y-axis):"
      ],
      "metadata": {
        "id": "qtz2TP9jqhB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(pres.predicted,y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-f72i-oQp6Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also get the predictions for an observation outside of our sample, e.g., consider an individual that is 180cm tall and weighs 85kg:"
      ],
      "metadata": {
        "id": "5ZwuKyHFqt3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pres = logit_mod_res.get_prediction([1,180,85])"
      ],
      "metadata": {
        "id": "rm_UjFQxP-yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pres.summary_frame()"
      ],
      "metadata": {
        "id": "0-gc-U7yRopg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, due to parameter error, our confidence band for the predicted probability is between 98.1% and 99.96%. So, clarly we would label this observation as $\\hat{y}=1$ (male). However, note that there is a chance we are wrong, and given parameter errors our chance of being wrong may be close to 1.9%."
      ],
      "metadata": {
        "id": "4w9z4LLlvlnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our Classifier: Confusion Table\n",
        "\n",
        "To evaluate our classifier, we can determine the confusion table for our predictions. Recall that the confusion table or matrix arranges the predicted class from left to right and the actual class on the vertical for a 2-by-2 table:\n",
        "\n",
        "$$\n",
        "\\begin{array}{|l|c|c|}\n",
        "\\hline\n",
        " \\text{Actual\\Predicted} &  \\text{False, } \\hat{y}=0 & \\text{True, } \\hat{y}=1\\\\\n",
        " \\hline\n",
        " \\text{False, } y=0 & \\text{TN} & \\text{FP} \\\\\n",
        " \\text{True, } y=1 & \\text{FN} & \\text{TN}\\\\\n",
        " \\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Often, one also includes the row and column sums.\n",
        "\n",
        "Let's take a look:"
      ],
      "metadata": {
        "id": "yuWWqE7btQKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = (logit_mod_res.predict(X) > 0.5)\n",
        "conf_mat = pd.crosstab(y['sex'], pred, rownames=['Actual Sex'], colnames=['Predicted Sex'])\n",
        "# Add row and column sums\n",
        "conf_mat.loc['Column_Total']= conf_mat.sum(numeric_only=True, axis=0)\n",
        "conf_mat.loc[:,'Row_Total'] = conf_mat.sum(numeric_only=True, axis=1)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "aquu9y4PtjcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we described, we can determine various metrics based on the confusion table:"
      ],
      "metadata": {
        "id": "Hg6XG77uwQNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TPR = 78 / 88 # True-Positive Rate\n",
        "TNR = 104 / 112 # True-Negative Rate\n",
        "FPR = 8 / 112 # False-Positive Rate = 1 - TNR\n",
        "FNR = 10 / 88 # False-Negative Rate = 1 - TPR\n",
        "MCR = (8+10)/200 # Miss Classification Rate\n",
        "ACC = (78+104)/200 # Accuracy\n",
        "print('TPR =', TPR)\n",
        "print('TNR =', TNR)\n",
        "print('FPR =', FPR)\n",
        "print('FNR =', FNR)\n",
        "print('MCR =', MCR)\n",
        "print('ACC =', ACC)"
      ],
      "metadata": {
        "id": "FIOqPmZxwMT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}